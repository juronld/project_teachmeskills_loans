Техническое задание на разработку системы анализа данных

1. Введение

Цель проекта: Разработать систему для анализа данных и решения аналитических задач в выбранной предметной области с использованием современных инструментов и методологий.

Задачи:
	Выбрать предметную область и определить аналитические задачи.
	Определить источники данных (открытые или вымышленные).
	Спроектировать и реализовать модель данных хранилища (DWH).
	Разработать и реализовать ETL-процессы с интеграцией источников.
	Обеспечить качество данных с использованием проверок и SCD2.
	Реализовать логирование ETL-процессов в схему tech.
	Построить витрины данных с учетом временных и ситуативных зависимостей.
	Создать дашборды для визуализации результатов.

2. Выбор предметной области и аналитических задач
	Самостоятельно выбрать предметную область (например, ритейл, финансы, здравоохранение, логистика).
	Определить 3-5 аналитических задач для решения с помощью системы.
	Для каждой задачи сформулировать бизнес-требования и метрики успеха.

	Пример для ритейла:
		Задача 1: Анализ продаж по регионам.
			Метрики: Объём продаж, средний чек, количество транзакций.
			Бизнес-требование: Фильтрация по дате, региону, категории товара.
		Задача 2: Прогнозирование спроса на товары.
			Метрики: Точность прогноза, объём спроса.
			Бизнес-требование: Ежемесячные прогнозы по категориям.
		Задача 3: Анализ поведения клиентов.
			Метрики: Частота покупок, удержание клиентов.
			Бизнес-требование: Сегментация клиентов по активности.

3. Источники данных
	Определить 2-3 источника данных (открытые датасеты или вымышленные).
	Указать для каждого источника:
	Тип (СУБД и CSV/JSON/API).
	Структуру данных (схема или формат).
	Объём данных.
	Частоту обновления.
	Для СУБД - на стороне источника должна быть создана техническая тех запись, через которую осуществляется интеграция с DWH 

	Пример:
		Источник 1: База данных продаж (PostgreSQL).
			Таблицы: sales, products, customers.
			Объём: 1 млн записей.
			Обновление: ежедневно.
		Источник 2: CSV-файлы с данными о поставках.
			Формат: CSV.
			Объём: 100 тыс. записей.
			Обновление: еженедельно.
4. Модель данных хранилища (DWH)
	Спроектировать многослойную архитектуру:
		Необходио отрисовать физическую и логическую модели.
		Stage: Сырые данные.
		DDS: Нормализованные данные с поддержкой SCD2.
		DM: Денормализованные витрины.
		Tech: Логирование процессов.
	Описать физическую структуру (таблицы, поля, типы данных, индексы).

5. Интеграция с источниками и ETL-процессы
	Разработать ETL-процессы с использованием Python и Airflow.
	Реализовать инкрементальную загрузку (по временным меткам или ID).
	Создать DAG для каждого источника с зависимостями задач (при наличии).
	В одном из источников реализовать статусную таблицу (готовность данных источника за конкретную дату для каждого объекта).

6. Проверки качества данных
	Реализовать проверки:
		Пропущенные значения.
		Дубликаты.
		Соответствие форматам.
		Бизнес-правила.
	Логировать результаты в схему tech.

7. Поддержка SCD2
	Реализовать SCD2 для измерений (например, customers, products).

8. Логирование ETL-процессов
	Создать схему tech в PostgreSQL с лог-таблицами.
	Минимальные требования к логируемым метрикам:
		Имя процесса.
		Имя загружаемого объекта.
		Время начала/окончания.
		Статус (успех/ошибка).
		Количество обработанных записей.
		Комментарий (например, для SCD2 - можно фиксировать количество Updates/Inserts/Deletes)

		Отдельно логируется информацию по качеству данных - пройдена ли проверка успешно, либо нет. Проверить все кейсы, подготовив синтетические данные (содержащие дубли, значения за допустимым диапазоном и тд)

9. Сборка витрин данных (Data Marts)
		Создать витрины в схеме витрин.
		Оптимизировать для аналитики.
10. Визуализация данных в Metabase
		Подключить BI-tool к DWH.
		Создать 2-3 дашборда для аналитических задач.
		Добавить графики, таблицы, фильтры.

11. Требования к разработке
		Использовать Git.
		Предоставить документацию и инструкцию по развертыванию.
12. Критерии приёмки
		Данные загружаются корректно.
		ETL-процессы работают по расписанию.
		Проверки качества и логирование функционируют.
		Витрины актуальны.
		Дашборды отображают данные согласно задачам.
		Код доступен в GIT
		Наличие документации проекта

